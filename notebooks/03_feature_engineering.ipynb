{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe17020",
   "metadata": {},
   "source": [
    "# Feature Engineering - House Price Dataset\n",
    "## Introduction\n",
    "This notebook focuses on **feature engineering** for the house price prediction project. Using the cleaned dataset from `02_data_preprocessing.ipynb`, we will create new meaningful features that can help improve model performance.\n",
    "\n",
    "**Dataset:** Housing Price Prediction Data (Kaggle)\n",
    "\n",
    "**Objective:** Create and select new features to improve model performance and prepare the dataset for modeling.\n",
    "\n",
    "**Author:** NGUYEN Ngoc Dang Nguyen - Final-year Student in Computer Science, Aix-Marseille University\n",
    "\n",
    "**Feature engineering steps:**\n",
    "1. Load cleaned data and understand feature relationships\n",
    "2. Create domain-specific features (housing market insights)\n",
    "3. Generate interaction features between important variables\n",
    "4. Create polynomial and mathematical transformations\n",
    "5. Implement binning and categorical feature creation\n",
    "6. Feature selection and importance analysis\n",
    "7. Save engineered dataset for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7960296",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from scipy.stats import skew\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"../data/processed/cleaned_data.csv\")\n",
    "\n",
    "print(f\"Cleaned dataset loaded: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\nSample of the cleaned data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568dcb2",
   "metadata": {},
   "source": [
    "## 2. Understanding Current Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def71ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCURRENT FEATURE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identify different types of features\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "encoded_features = [col for col in df.columns if col.endswith(('_Rural', '_Suburb', '_Urban'))]\n",
    "scaled_features = [col for col in df.columns if col.endswith('_scaled')]\n",
    "\n",
    "print(f\"Feature inventory:\")\n",
    "print(f\"    * Total features: {len(df.columns)}\")\n",
    "print(f\"    * Numeric features: {len(numeric_features)}\")\n",
    "print(f\"    * Categorical features: {len(categorical_features)}\")\n",
    "print(f\"    * Encoded features: {len(encoded_features)}\")\n",
    "print(f\"    * Scaled features: {len(scaled_features)}\")\n",
    "\n",
    "# Find the target variable (price)\n",
    "price_cols = [col for col in numeric_features if 'price' in col.lower() and not col.endswith('_scaled')]\n",
    "target_col = price_cols[0] if price_cols else numeric_features[0]\n",
    "\n",
    "print(f\"\\nTarget variable identified: {target_col}\")\n",
    "\n",
    "# Show the correlation with target\n",
    "correlations = df[numeric_features].corr()[target_col].abs().sort_values(ascending=False)\n",
    "print(f\"\\nTop 5 features correlated with {target_col}:\")\n",
    "for feature, corr in correlations.head(6).items():\n",
    "    if feature != target_col:\n",
    "        print(f\"    * {feature}: {corr:.3f}\")\n",
    "\n",
    "# Basic feature statistics\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(df[numeric_features].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7275",
   "metadata": {},
   "source": [
    "## 3. Domain-Specific Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDOMAIN-SPECIFIC FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Keep track of new features created\n",
    "new_features = []\n",
    "\n",
    "# Living Space Features\n",
    "print(\"Creating living space features...\")\n",
    "\n",
    "# Total living area features\n",
    "living_cols = [col for col in df.columns if 'sqft' in col.lower() or 'area' in col.lower()]\n",
    "if len(living_cols) >= 2:\n",
    "    # Assume we have sqft_living and sqft_lot or similar\n",
    "    for i, col1 in enumerate(living_cols):\n",
    "        for col2 in living_cols[i+1:]:\n",
    "            if col1 != col2 and not col1.endswith('_scaled') and not col2.endswith('_scaled'):\n",
    "                # Create ratio feature\n",
    "                ratio_name = f\"{col1}_to_{col2}_ratio\"\n",
    "                df[ratio_name] = df[col1] / (df[col2] + 1)  # Add 1 to avoid division by zero\n",
    "                new_features.append(ratio_name)\n",
    "                print(f\"  Created {ratio_name}\")\n",
    "\n",
    "# Room density features\n",
    "room_cols = [col for col in df.columns if 'room' in col.lower() or 'bedroom' in col.lower() or 'bathroom' in col.lower()]\n",
    "area_cols = [col for col in df.columns if 'sqft' in col.lower() and 'living' in col.lower()]\n",
    "\n",
    "if room_cols and area_cols:\n",
    "    for room_col in room_cols:\n",
    "        for area_col in area_cols:\n",
    "            if not room_col.endswith('_scaled') and not area_col.endswith('_scaled'):\n",
    "                density_name = f\"{room_col}_per_sqft\"\n",
    "                df[density_name] = df[room_col] / (df[area_col] + 1)\n",
    "                new_features.append(density_name)\n",
    "                print(f\"  Created {density_name}\")\n",
    "\n",
    "# 3.2 Age and Time Features\n",
    "print(f\"\\nCreating age and time features...\")\n",
    "\n",
    "# House age features\n",
    "year_cols = [col for col in df.columns if 'year' in col.lower() or 'built' in col.lower()]\n",
    "for col in year_cols:\n",
    "    if not col.endswith('_scaled') and df[col].dtype in [np.number, 'int64', 'float64']:\n",
    "        # Current age\n",
    "        age_col = f\"{col}_age\"\n",
    "        df[age_col] = 2024 - df[col]\n",
    "        new_features.append(age_col)\n",
    "        print(f\"  Created {age_col}\")\n",
    "        \n",
    "        # Age categories\n",
    "        age_cat_col = f\"{col}_category\"\n",
    "        df[age_cat_col] = pd.cut(df[age_col], \n",
    "                                bins=[0, 10, 25, 50, 100, float('inf')], \n",
    "                                labels=['New', 'Recent', 'Mature', 'Old', 'Historic'])\n",
    "        new_features.append(age_cat_col)\n",
    "        print(f\"  Created {age_cat_col}\")\n",
    "\n",
    "# 3.3 Location and Neighborhood Features\n",
    "print(f\"\\nCreating location-based features...\")\n",
    "\n",
    "# ZIP code or location features\n",
    "location_cols = [col for col in df.columns if any(word in col.lower() for word in ['zip', 'location', 'city', 'neighborhood'])]\n",
    "for col in location_cols:\n",
    "    if df[col].dtype == 'object' or col.endswith('_encoded'):\n",
    "        # Average price by location\n",
    "        location_avg_col = f\"{col}_avg_price\"\n",
    "        location_avg = df.groupby(col)[target_col].mean()\n",
    "        df[location_avg_col] = df[col].map(location_avg)\n",
    "        new_features.append(location_avg_col)\n",
    "        print(f\"  Created {location_avg_col}\")\n",
    "\n",
    "# 3.4 Price-based Features\n",
    "print(f\"\\nCreating price-related features...\")\n",
    "\n",
    "# Price per unit features\n",
    "size_cols = [col for col in df.columns if any(word in col.lower() for word in ['sqft', 'area', 'size'])]\n",
    "for col in size_cols:\n",
    "    if not col.endswith('_scaled') and df[col].dtype in [np.number, 'int64', 'float64']:\n",
    "        price_per_unit = f\"price_per_{col}\"\n",
    "        df[price_per_unit] = df[target_col] / (df[col] + 1)\n",
    "        new_features.append(price_per_unit)\n",
    "        print(f\"  Created {price_per_unit}\")\n",
    "\n",
    "print(f\"\\nDomain features created: {len([f for f in new_features if f in df.columns])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b8030",
   "metadata": {},
   "source": [
    "## 4. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nINTERACTION FEATURE CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Find top correlated features for interactions\n",
    "base_numeric = [col for col in numeric_features if not col.endswith('_scaled') and col != target_col]\n",
    "top_features = correlations.drop(target_col).head(5).index.tolist()\n",
    "top_features = [f for f in top_features if f in base_numeric]\n",
    "\n",
    "print(f\"Creating interactions between top {len(top_features)} features: {top_features}\")\n",
    "\n",
    "interaction_features = []\n",
    "\n",
    "# Create pairwise interactions\n",
    "for i, feat1 in enumerate(top_features):\n",
    "    for feat2 in top_features[i+1:]:\n",
    "        if feat1 != feat2:\n",
    "            # Multiplication interaction\n",
    "            mult_name = f\"{feat1}_x_{feat2}\"\n",
    "            df[mult_name] = df[feat1] * df[feat2]\n",
    "            interaction_features.append(mult_name)\n",
    "            \n",
    "            # Addition interaction\n",
    "            add_name = f\"{feat1}_plus_{feat2}\"\n",
    "            df[add_name] = df[feat1] + df[feat2]\n",
    "            interaction_features.append(add_name)\n",
    "            \n",
    "            # Ratio interaction (if no zeros)\n",
    "            if df[feat2].min() > 0:\n",
    "                ratio_name = f\"{feat1}_div_{feat2}\"\n",
    "                df[ratio_name] = df[feat1] / df[feat2]\n",
    "                interaction_features.append(ratio_name)\n",
    "\n",
    "print(f\"Created {len(interaction_features)} interaction features\")\n",
    "\n",
    "# Show sample interactions\n",
    "if interaction_features:\n",
    "    print(f\"\\nSample interactions:\")\n",
    "    for feat in interaction_features[:5]:\n",
    "        print(f\"  â€¢ {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14efd3",
   "metadata": {},
   "source": [
    "## 5. Mathematical Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97842ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMATHEMATICAL TRANSFORMATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "transformation_features = []\n",
    "\n",
    "# Polynomial features for top predictors\n",
    "print(\"Creating polynomial features...\")\n",
    "\n",
    "poly_candidates = [col for col in top_features if df[col].min() >= 0]\n",
    "\n",
    "for col in poly_candidates[:3]:\n",
    "    # Square\n",
    "    square_name = f\"{col}_squared\"\n",
    "    df[square_name] = df[col] ** 2\n",
    "    transformation_features.append(square_name)\n",
    "\n",
    "    # Square root (if all values are non-negative)\n",
    "    if df[col].min() >= 0:\n",
    "        sqrt_name = f\"{col}_sqrt\"\n",
    "        df[sqrt_name] = np.log(df[col])\n",
    "        transformation_features.append(sqrt_name)\n",
    "    \n",
    "    # Log transformation (if all values are positive)\n",
    "    if df[col].min() > 0:\n",
    "        log_name = f\"{col}_log\"\n",
    "        df[log_name] = np.log(df[col])\n",
    "        transformation_features.append(log_name)\n",
    "    \n",
    "print(f\"Created {len(transformation_features)} mathematical transformations\")\n",
    "\n",
    "# Binning continuous variables\n",
    "print(f\"\\nCreating binned cartegorical features...\")\n",
    "\n",
    "binning_features = []\n",
    "\n",
    "# Bin important continuous features\n",
    "for col in top_features[:3]:\n",
    "    if df[col].dtype in [np.number, 'int64', 'float64'] and df[col].nunique() > 10:\n",
    "        # Create quantile-based bins\n",
    "        binned_name = f\"{col}_binned\"\n",
    "        df[binned_name] = pd.qcut(df[col], q=5, labels=['Low', 'Low-Med', 'Medium', 'Med-High', 'High'])\n",
    "        binning_features.append(binned_name)\n",
    "        print(f\"  Created {binned_name}\")\n",
    "\n",
    "print(f\"Created {len(binning_features)} binned features\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6085e2",
   "metadata": {},
   "source": [
    "## 6. Feature Selection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf20a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFEATURE SELECTION AND ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get all features for analysis\n",
    "all_numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in all_numeric_features:\n",
    "    all_numeric_features.remove(target_col)\n",
    "\n",
    "# Remove scaled features for now (we'll add them back later)\n",
    "analysis_features = [col for col in all_numeric_features if not col.endswith('_scaled')]\n",
    "\n",
    "print(f\"Analyzing {len(analysis_features)} features for selection\")\n",
    "\n",
    "# Calculate correlations with target\n",
    "feature_correlations = df[analysis_features + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
    "feature_correlations = feature_correlations.drop(target_col)\n",
    "\n",
    "print(f\"\\nTop 10 most correlated features with {target_col}:\")\n",
    "for i, (feature, corr) in enumerate(feature_correlations.head(10).items(), 1):\n",
    "    feature_type = \"New\" if feature in new_features + interaction_features + transformation_features + binning_features else \"Original\"\n",
    "    print(f\"{i:2d}. {feature:<30} | {corr:.3f} | {feature_type}\")\n",
    "\n",
    "# Visualize top feature correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features_viz = feature_correlations.head(15)\n",
    "plt.barh(range(len(top_features_viz)), top_features_viz.values)\n",
    "plt.yticks(range(len(top_features_viz)), top_features_viz.index)\n",
    "plt.xlabel('Correlation with Price (absolute value)')\n",
    "plt.title('Top 15 Features by Correlation with Price')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical feature selection using SelectKBest\n",
    "print(f\"\\nStatistical feature selection (SelectKBest):\")\n",
    "selector = SelectKBest(score_func=f_regression, k=20)  # Select top 20 features\n",
    "X_selected = selector.fit_transform(df[analysis_features], df[target_col])\n",
    "selected_features = [analysis_features[i] for i in selector.get_support(indices=True)]\n",
    "\n",
    "print(f\"SelectKBest selected {len(selected_features)} features:\")\n",
    "for i, feature in enumerate(selected_features[:10], 1):  # Show top 10\n",
    "    feature_type = \"New\" if feature in new_features + interaction_features + transformation_features + binning_features else \"Original\"\n",
    "    print(f\"{i:2d}. {feature:<30} | {feature_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7b228",
   "metadata": {},
   "source": [
    "## 7. Final Feature Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c449aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL FEATURE SET CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Combine best features for modeling\n",
    "final_features = []\n",
    "\n",
    "# 1. Top correlated features\n",
    "final_features.extend(feature_correlations.head(15).index.tolist())\n",
    "\n",
    "# 2. Top statistically selected features\n",
    "final_features.extend(selected_features[:10])\n",
    "\n",
    "# 3. Remove duplicates and add scaled versions\n",
    "final_features = list(set(final_features))\n",
    "\n",
    "# Add scaled versions of numerical features for ML algorithms\n",
    "scaled_versions = []\n",
    "for feature in final_features:\n",
    "    scaled_name = f\"{feature}_scaled\"\n",
    "    if scaled_name in df.columns:\n",
    "        scaled_versions.append(scaled_name)\n",
    "\n",
    "# Create final modeling dataset\n",
    "modeling_features = final_features + scaled_versions + [target_col]\n",
    "modeling_df = df[modeling_features].copy()\n",
    "\n",
    "print(f\"Final feature set summary:\")\n",
    "print(f\"  * Original features: {len([f for f in final_features if f not in new_features + interaction_features + transformation_features])}\")\n",
    "print(f\"  * Engineered features: {len([f for f in final_features if f in new_features + interaction_features + transformation_features])}\")\n",
    "print(f\"  * Scaled versions: {len(scaled_versions)}\")\n",
    "print(f\"  * Total features for modeling: {len(modeling_features) - 1}\")  # Subtract target\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_in_final = modeling_df.isnull().sum()\n",
    "if missing_in_final.sum() > 0:\n",
    "    print(f\"\\nMissing values in final dataset:\")\n",
    "    print(missing_in_final[missing_in_final > 0])\n",
    "else:\n",
    "    print(f\"\\nNo missing values in final dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5c8f3",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa84eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFEATURE ENGINEERING VALIDATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Correlation heatmap of top features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features_for_viz = feature_correlations.head(12).index.tolist() + [target_col]\n",
    "correlation_matrix = df[top_features_for_viz].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation Matrix - Top Engineered Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "top_new_features = [f for f in feature_correlations.head(10).index if f in new_features + interaction_features + transformation_features][:6]\n",
    "\n",
    "for i, feature in enumerate(top_new_features):\n",
    "    if i < 6:\n",
    "        df[feature].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d74447",
   "metadata": {},
   "source": [
    "## 9. Save Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSAVING ENGINEERED DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save the complete engineered dataset\n",
    "engineered_path = '../data/processed/engineered_data.csv'\n",
    "df.to_csv(engineered_path, index=False)\n",
    "print(f\"Complete engineered dataset saved: {engineered_path}\")\n",
    "print(f\"Size: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "\n",
    "# Save the modeling-ready dataset\n",
    "modeling_path = '../data/processed/modeling_data.csv'\n",
    "modeling_df.to_csv(modeling_path, index=False)\n",
    "print(f\"Modeling dataset saved: {modeling_path}\")\n",
    "print(f\"Size: {modeling_df.shape[0]} rows x {modeling_df.shape[1]} columns\")\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'target_column': target_col,\n",
    "    'total_features': len(df.columns) - 1,  # Exclude target\n",
    "    'original_features': len([f for f in df.columns if f not in new_features + interaction_features + transformation_features + binning_features]),\n",
    "    'domain_features': len(new_features),\n",
    "    'interaction_features': len(interaction_features),\n",
    "    'transformation_features': len(transformation_features),\n",
    "    'binning_features': len(binning_features),\n",
    "    'final_modeling_features': len(modeling_features) - 1,\n",
    "    'top_features': feature_correlations.head(10).to_dict(),\n",
    "    'selected_features_for_modeling': [f for f in modeling_features if f != target_col]\n",
    "}\n",
    "\n",
    "# Save feature info\n",
    "import json\n",
    "with open('../data/processed/feature_engineering_summary.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Feature engineering summary saved: '../data/processed/feature_engineering_summary.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476be3ab",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary & Next Steps\n",
    "Through domain-driven feature creation, interaction terms, and careful selection, the dataset now contains informative variables that enhance model learning. The engineered features are ready for use in training and evaluating machine learning models, setting the stage for improved predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
