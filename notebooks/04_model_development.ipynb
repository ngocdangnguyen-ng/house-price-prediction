{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d154319d",
   "metadata": {},
   "source": [
    "# Model Development - House Price Dataset\n",
    "## Introduction\n",
    "This notebook develops and compares multiple machine learning models for house price prediction. Using the engineered features from `03_feature_engineering.ipynb`, I will train, evaluate, and select the best performing model.\n",
    "\n",
    "**Dataset:** Housing Price Prediction Data (Kaggle)\n",
    "\n",
    "**Objective:** Train, tune, and compare regression models to predict house prices as accurately as possible.\n",
    "\n",
    "**Author:** NGUYEN Ngoc Dang Nguyen - Final-year Student in Computer Science, Aix-Marseille University\n",
    "\n",
    "**Model development steps:**\n",
    "1. Load engineered data and prepare for modeling\n",
    "2. Split data into training and testing sets\n",
    "3. Train multiple regression algorithms\n",
    "4. Evaluate model performance using multiple metrics\n",
    "5. Compare models and select the best one\n",
    "6. Analyze feature importance and model for deployment\n",
    "7. Save the final model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c860da2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the engineered dataset\n",
    "df = pd.read_csv(\"../data/processed/modeling_data.csv\")\n",
    "\n",
    "print(f\"Modeling dataset loaded: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Load feature engineering summary to get target column and display\n",
    "import json\n",
    "with open('../data/processed/feature_engineering_summary.json', 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "\n",
    "target_col = feature_info['target_column']\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "print(f\"\\nSample of the cleaned data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d26f71",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36209a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPARING DATA FOR MODELING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Dataset summary:\")\n",
    "print(f\"    * Features (X): {X.shape[1]} columns\")\n",
    "print(f\"    * Target (y): {y.shape[0]} values\")\n",
    "print(f\"    * Target range: {y.min():.2f} to {y.max():.2f}\")\n",
    "print(f\"    * Target mean: {y.mean():.2f}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"    * Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"    * Missing values in y: {y.isnull().sum()}\")\n",
    "print(f\"    * Infinite values in X: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Handle any infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"Found {X.isnull().sum().sum()} missing/infinite values - filling with median\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Feature types analysis\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"    * Numeric features: {len(numeric_features)}\")\n",
    "print(f\"    * Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Handle categorical features if any\n",
    "if categorical_features:\n",
    "    print(f\"  Encoding categorical features...\")\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "    print(f\"  After encoding: {X_encoded.shape[1]} features\")\n",
    "    X = X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23c430",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a95b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSPLITTING DATA FOR TRAINING AND TESTING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Data split summary:\")\n",
    "print(f\"  * Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  * Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  * Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check target distribution in splits\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  * Training mean: {y_train.mean():.2f}\")\n",
    "print(f\"  * Test mean: {y_test.mean():.2f}\")\n",
    "print(f\"  * Training std: {y_train.std():.2f}\")\n",
    "print(f\"  * Test std: {y_test.std():.2f}\")\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y_train, bins=30, alpha=0.7, label='Train', color='blue')\n",
    "plt.hist(y_test, bins=30, alpha=0.7, label='Test', color='orange')\n",
    "plt.title('Target Distribution: Train vs Test')\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(range(len(y_train)), sorted(y_train), alpha=0.6, s=1, label='Train')\n",
    "plt.scatter(range(len(y_test)), sorted(y_test), alpha=0.6, s=1, label='Test')\n",
    "plt.title('Target Values Distribution')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel(target_col)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot([y_train, y_test], labels=['Train', 'Test'])\n",
    "plt.title('Target Distribution Comparison')\n",
    "plt.ylabel(target_col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3c459",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44753b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMODEL TRAINING AND COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Training models...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'CV_RMSE': cv_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"  {name} completed:\")\n",
    "    print(f\"     * Test R²: {test_r2:.3f}\")\n",
    "    print(f\"     * Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"     * Test MAE: {test_mae:.2f}\")\n",
    "\n",
    "print(f\"\\nAll {len(models)} models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91329a3",
   "metadata": {},
   "source": [
    "## 5. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "print(\"Complete Results Table:\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df['Test_R2'].idxmax()\n",
    "best_model = trained_models[best_model_name]\n",
    "best_r2 = results_df.loc[best_model_name, 'Test_R2']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"   * Test R²: {best_r2:.3f}\")\n",
    "print(f\"   * Test RMSE: {results_df.loc[best_model_name, 'Test_RMSE']:.2f}\")\n",
    "print(f\"   * Test MAE: {results_df.loc[best_model_name, 'Test_MAE']:.2f}\")\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R² Score comparison\n",
    "axes[0, 0].bar(results_df.index, results_df['Test_R2'], color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('Model Comparison - R² Score')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].bar(results_df.index, results_df['Test_RMSE'], color='lightcoral', alpha=0.8)\n",
    "axes[0, 1].set_title('Model Comparison - RMSE')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training vs Test R²\n",
    "train_r2 = results_df['Train_R2']\n",
    "test_r2 = results_df['Test_R2']\n",
    "x_pos = np.arange(len(results_df))\n",
    "\n",
    "axes[1, 0].bar(x_pos - 0.2, train_r2, 0.4, label='Train R²', alpha=0.8)\n",
    "axes[1, 0].bar(x_pos + 0.2, test_r2, 0.4, label='Test R²', alpha=0.8)\n",
    "axes[1, 0].set_title('Train vs Test R² Score')\n",
    "axes[1, 0].set_ylabel('R² Score')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(results_df.index, rotation=45)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation RMSE\n",
    "axes[1, 1].bar(results_df.index, results_df['CV_RMSE'], color='lightgreen', alpha=0.8)\n",
    "axes[1, 1].set_title('Cross-Validation RMSE')\n",
    "axes[1, 1].set_ylabel('CV RMSE')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0af47",
   "metadata": {},
   "source": [
    "## 6. Best Model Analysis and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9600b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nANALYZING BEST MODEL: {best_model_name}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make predictions with best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Detailed performance analysis\n",
    "print(f\"Detailed Performance Metrics:\")\n",
    "print(f\"  * Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"  * Root Mean Square Error: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
    "print(f\"  * R² Score: {r2_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  * Mean Absolute Percentage Error: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"\\nFeature Importance Analysis:\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']:<30} | {row['importance']:.3f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    print(f\"\\nLinear Model Coefficients Analysis:\")\n",
    "    \n",
    "    # Get coefficients for linear models\n",
    "    coef = best_model.coef_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create coefficients DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coef,\n",
    "        'abs_coefficient': np.abs(coef)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Features by Coefficient Magnitude:\")\n",
    "    for i, (_, row) in enumerate(coef_df.head(10).iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']:<30} | {row['coefficient']:>8.3f}\")\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = coef_df.head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_features['coefficient']]\n",
    "    plt.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top 15 Feature Coefficients - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83214700",
   "metadata": {},
   "source": [
    "## 7. Model Validation and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMODEL VALIDATION AND ERROR ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prediction vs Actual plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(f'Actual {target_col}')\n",
    "plt.ylabel(f'Predicted {target_col}')\n",
    "plt.title('Predicted vs Actual Values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(f'Predicted {target_col}')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error statistics\n",
    "print(f\"Error Analysis:\")\n",
    "print(f\"  * Mean Residual: {residuals.mean():.3f}\")\n",
    "print(f\"  * Std of Residuals: {residuals.std():.3f}\")\n",
    "print(f\"  * Min Residual: {residuals.min():.2f}\")\n",
    "print(f\"  * Max Residual: {residuals.max():.2f}\")\n",
    "\n",
    "# Percentage of predictions within certain error ranges\n",
    "error_percentages = {}\n",
    "for pct in [5, 10, 15, 20]:\n",
    "    threshold = y_test.mean() * (pct / 100)\n",
    "    within_threshold = (np.abs(residuals) <= threshold).sum()\n",
    "    error_percentages[pct] = (within_threshold / len(y_test)) * 100\n",
    "\n",
    "print(f\"\\nPrediction Accuracy:\")\n",
    "for pct, accuracy in error_percentages.items():\n",
    "    print(f\"  * Within {pct}% of actual: {accuracy:.1f}% of predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0d1c4",
   "metadata": {},
   "source": [
    "## 8. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSAVING THE FINAL MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = f'../models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model saved: {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': str(type(best_model).__name__),\n",
    "    'test_r2_score': float(best_r2),\n",
    "    'test_rmse': float(results_df.loc[best_model_name, 'Test_RMSE']),\n",
    "    'test_mae': float(results_df.loc[best_model_name, 'Test_MAE']),\n",
    "    'feature_count': int(X.shape[1]),\n",
    "    'training_samples': int(len(X_train)),\n",
    "    'test_samples': int(len(X_test)),\n",
    "    'target_column': target_col,\n",
    "    'features_used': X.columns.tolist()\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open('../models/model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model metadata saved: '../models/model_metadata.json'\")\n",
    "\n",
    "# Save performance results\n",
    "results_df.to_csv('../models/model_comparison_results.csv')\n",
    "print(f\"Comparison results saved: '../models/model_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efda1a",
   "metadata": {},
   "source": [
    "## Model Development Summary\n",
    "Multiple machine learning models were trained, tuned, and compared using robust evaluation metrics. The best-performing model was selected based on validation results and error analysis. This model, along with insights into feature importance and error patterns, provides a strong foundation for final evaluation and deployment in the next stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
