{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86697d39",
   "metadata": {},
   "source": [
    "# Data Preprocessing - House Price Dataset\n",
    "## Introduction\n",
    "\n",
    "This notebook performs **data preprocessing** for the residential housing dataset analyzed in `01_exploratory_data_analysis.ipynb`. Based on the insights from the exploratory phase, we will clean and transform the raw dataset to make it suitable for feature engineering and predictive modeling.\n",
    "\n",
    "**Dataset:** Housing Price Prediction Data (Kaggle)\n",
    "\n",
    "**Objective:** Produce a clean dataset stored in `data/processed/cleaned_data.csv` that can be directly used in the feature engineering stage.\n",
    "\n",
    "**Author:** NGUYEN Ngoc Dang Nguyen – Final-year Student in Computer Science, Aix-Marseille University\n",
    "\n",
    "**Preprocessing steps:**\n",
    "1. Import libraries and load the raw data\n",
    "2. Assess dataset structure and initial data quality\n",
    "3. Analyze and handle missing values\n",
    "4. Detect and treat outliers\n",
    "5. Standardize data types\n",
    "6. Create basic derived features\n",
    "7. Encode categorical variables\n",
    "8. Scale numerical features\n",
    "9. Final data quality check\n",
    "10. Save the processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578c902",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d815f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the raw data\n",
    "df = pd.read_csv(\"../data/raw/housing_price_dataset.csv\")\n",
    "df.columns = df.columns.str.strip() \n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Keep a copy of original data for comparison\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f\"\\nSample of the data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a9c60",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview & Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2166a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial Data Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic info\n",
    "print(f\"Dataset dimensions: {df.shape}\")\n",
    "print(f\"Data types:\\n{df.dtypes.value_counts()}\")\n",
    "\n",
    "# Memory and data quality check\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"Total cells: {df.shape[0] * df.shape[1]:,}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum():,} ({df.isnull().sum().sum()/(df.shape[0] * df.shape[1])*100:.1f}%)\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum():,}\")\n",
    "\n",
    "# Quick data overview\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614e844",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis & Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693393f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMISSING VALUES HANDLING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"Found {len(missing_summary)} columns with missing values:\")\n",
    "    print(missing_summary.round(2).to_string(index=False))\n",
    "    \n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=missing_summary.head(10), y='Column', x='Missing_Percentage')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xlabel('Percentage Missing (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Handle missing values column by column\n",
    "    print(f\"\\nHandling missing values:\")\n",
    "    \n",
    "    for _, row in missing_summary.iterrows():\n",
    "        col = row['Column']\n",
    "        missing_pct = row['Missing_Percentage']\n",
    "        \n",
    "        if df[col].dtype in ['object']:\n",
    "            # Categorical columns - fill with mode or 'Unknown'\n",
    "            if missing_pct < 50:  # If less than 50% missing, use mode\n",
    "                mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                print(f\"  * {col}: filled {row['Missing_Count']} missing values with '{mode_val}'\")\n",
    "            else:\n",
    "                df[col] = df[col].fillna('Unknown')\n",
    "                print(f\"  * {col}: filled {row['Missing_Count']} missing values with 'Unknown'\")\n",
    "        \n",
    "        else:\n",
    "            # Numerical columns - fill with median or mean\n",
    "            if missing_pct < 50:\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "                print(f\"  {col}: filled {row['Missing_Count']} missing values with median ({median_val:.2f})\")\n",
    "            else:\n",
    "                # Too many missing values - consider dropping the column\n",
    "                print(f\"  {col}: {missing_pct:.1f}% missing - consider dropping this column\")\n",
    "\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Verify missing values are handled\n",
    "remaining_missing = df.isnull().sum().sum()\n",
    "print(f\"\\nMissing values after cleaning: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58ad4",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOUTLIER DETECTION & TREATMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Find numerical columns (likely price column)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numerical columns to check: {numeric_cols}\")\n",
    "\n",
    "# Find price column\n",
    "price_col = None\n",
    "for col in ['price', 'Price', 'price_per_m2', 'sale_price']:\n",
    "    if col in df.columns:\n",
    "        price_col = col\n",
    "        break\n",
    "\n",
    "if price_col is None and numeric_cols:\n",
    "    price_col = numeric_cols[0]  # Use first numeric column\n",
    "\n",
    "if price_col:\n",
    "    print(f\"\\nAnalyzing outliers in '{price_col}':\")\n",
    "    \n",
    "    # Calculate IQR bounds\n",
    "    Q1 = df[price_col].quantile(0.25)\n",
    "    Q3 = df[price_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers_mask = (df[price_col] < lower_bound) | (df[price_col] > upper_bound)\n",
    "    outliers_count = outliers_mask.sum()\n",
    "    \n",
    "    print(f\"Price statistics:\")\n",
    "    print(f\"  * Q1 (25%): {Q1:,.2f}\")\n",
    "    print(f\"  * Q3 (75%): {Q3:,.2f}\")\n",
    "    print(f\"  * IQR: {IQR:,.2f}\")\n",
    "    print(f\"  * Lower bound: {lower_bound:,.2f}\")\n",
    "    print(f\"  * Upper bound: {upper_bound:,.2f}\")\n",
    "    print(f\"  * Outliers found: {outliers_count} ({outliers_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize outliers\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.boxplot(df[price_col])\n",
    "    plt.title(f'{price_col} - Before Outlier Treatment')\n",
    "    plt.ylabel(price_col)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(df[price_col], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(lower_bound, color='red', linestyle='--', label='Lower bound')\n",
    "    plt.axvline(upper_bound, color='red', linestyle='--', label='Upper bound')\n",
    "    plt.title('Price Distribution with Outlier Bounds')\n",
    "    plt.xlabel(price_col)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Decision on outlier treatment\n",
    "    if outliers_count > len(df) * 0.1:  # More than 10% outliers\n",
    "        print(f\"  Too many outliers ({outliers_count/len(df)*100:.1f}%) - keeping them\")\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.text(0.5, 0.5, f'Keeping outliers\\n({outliers_count} values)\\nToo many to remove', \n",
    "                 ha='center', va='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "        plt.title('Decision: Keep Outliers')\n",
    "    else:\n",
    "        # Remove extreme outliers (optional - comment out if you want to keep them)\n",
    "        print(f\"  Removing {outliers_count} extreme outliers\")\n",
    "        df = df[~outliers_mask].copy()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.boxplot(df[price_col])\n",
    "        plt.title(f'{price_col} - After Outlier Removal')\n",
    "        plt.ylabel(price_col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Dataset size after outlier treatment: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172dafc",
   "metadata": {},
   "source": [
    "## 5. Data Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaaf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDATA TYPE STANDARDIZATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"Current data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert obvious numeric columns that might be stored as strings\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        # Try to convert to numeric if it looks like numbers\n",
    "        if df[col].str.contains(r'^[\\d\\.\\,\\-\\+]+$', regex=True, na=False).any():\n",
    "            try:\n",
    "                # Remove commas and convert to numeric\n",
    "                df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='ignore')\n",
    "                print(f\"  Converted {col} to numeric\")\n",
    "            except:\n",
    "                print(f\"  Could not convert {col} to numeric\")\n",
    "\n",
    "# Check for date columns\n",
    "date_keywords = ['date', 'year', 'time']\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in date_keywords):\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                print(f\"  Converted {col} to datetime\")\n",
    "            except:\n",
    "                print(f\"  Could not convert {col} to datetime\")\n",
    "\n",
    "print(f\"\\nData types after standardization:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c846590",
   "metadata": {},
   "source": [
    "## 6. Feature Creation & Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ab6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBASIC FEATURE CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create some simple derived features based on common house price factors\n",
    "features_created = []\n",
    "\n",
    "# Example feature engineering (adjust based on your actual columns)\n",
    "if 'bedrooms' in df.columns and 'bathrooms' in df.columns:\n",
    "    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n",
    "    features_created.append('total_rooms')\n",
    "\n",
    "if 'sqft_living' in df.columns and price_col in df.columns:\n",
    "    df['price_per_sqft'] = df[price_col] / df['sqft_living']\n",
    "    features_created.append('price_per_sqft')\n",
    "\n",
    "if 'yr_built' in df.columns:\n",
    "    df['house_age'] = 2024 - df['yr_built']\n",
    "    features_created.append('house_age')\n",
    "\n",
    "if 'sqft_lot' in df.columns and 'sqft_living' in df.columns:\n",
    "    df['lot_to_living_ratio'] = df['sqft_lot'] / df['sqft_living']\n",
    "    features_created.append('lot_to_living_ratio')\n",
    "\n",
    "if features_created:\n",
    "    print(f\"Created {len(features_created)} new features:\")\n",
    "    for feature in features_created:\n",
    "        print(f\"  * {feature}\")\n",
    "else:\n",
    "    print(\"No additional features created (depends on available columns)\")\n",
    "\n",
    "print(f\"\\nDataset now has {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd22ae0",
   "metadata": {},
   "source": [
    "## 7. Categorical Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCATEGORICAL VARIABLE ENCODING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns found: {categorical_cols}\")\n",
    "\n",
    "# Store encoders for later use\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"\\nEncoding '{col}' ({unique_count} unique values):\")\n",
    "    \n",
    "    if unique_count == 2:\n",
    "        # Binary encoding for binary categorical variables\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "        encoders[col] = le\n",
    "        print(f\"  Applied Label Encoding → {col}_encoded\")\n",
    "        \n",
    "    elif unique_count <= 10:\n",
    "        # One-hot encoding for low cardinality\n",
    "        dummies = pd.get_dummies(df[col], prefix=col)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        print(f\"Applied One-Hot Encoding → {dummies.columns.tolist()}\")\n",
    "        \n",
    "    else:\n",
    "        # Label encoding for high cardinality\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "        encoders[col] = le\n",
    "        print(f\"Applied Label Encoding → {col}_encoded (high cardinality)\")\n",
    "\n",
    "print(f\"\\nDataset after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0d403",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFEATURE SCALING PREPARATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identify numerical columns for scaling\n",
    "numeric_cols_for_scaling = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove encoded categorical columns from scaling (they're already 0/1)\n",
    "encoded_cols = [col for col in df.columns if col.endswith('_encoded') or \n",
    "                any(cat_col in col for cat_col in categorical_cols)]\n",
    "numeric_cols_for_scaling = [col for col in numeric_cols_for_scaling if col not in encoded_cols]\n",
    "\n",
    "print(f\"Columns that will need scaling for ML models: {len(numeric_cols_for_scaling)}\")\n",
    "print(f\"Columns: {numeric_cols_for_scaling}\")\n",
    "\n",
    "# Create scaled versions but keep original columns too\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[numeric_cols_for_scaling])\n",
    "\n",
    "# Add scaled columns with '_scaled' suffix\n",
    "scaled_df = pd.DataFrame(scaled_data, \n",
    "                        columns=[f\"{col}_scaled\" for col in numeric_cols_for_scaling],\n",
    "                        index=df.index)\n",
    "\n",
    "df = pd.concat([df, scaled_df], axis=1)\n",
    "print(f\"Added {len(numeric_cols_for_scaling)} scaled features\")\n",
    "\n",
    "# Store scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"Scaler saved to '../models/scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db31a7",
   "metadata": {},
   "source": [
    "## 9. Data Quality Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL DATA QUALITY CHECK\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Final dataset summary:\")\n",
    "print(f\"  * Rows: {df.shape[0]:,}\")\n",
    "print(f\"  * Columns: {df.shape[1]:,}\")\n",
    "print(f\"  * Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"  * Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"  * Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\nColumn types summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nSample of processed data:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b51647",
   "metadata": {},
   "source": [
    "## 10. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSAVING PROCESSED DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_path = '../data/processed/cleaned_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"Saved dataset: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Also save a summary of preprocessing steps\n",
    "preprocessing_summary = {\n",
    "    'original_shape': df_original.shape,\n",
    "    'final_shape': df.shape,\n",
    "    'missing_values_handled': df_original.isnull().sum().sum(),\n",
    "    'outliers_removed': len(df_original) - len(df),\n",
    "    'features_created': features_created,\n",
    "    'categorical_columns_encoded': categorical_cols,\n",
    "    'features_scaled': numeric_cols_for_scaling\n",
    "}\n",
    "\n",
    "# Save preprocessing summary\n",
    "import json\n",
    "with open('../data/processed/preprocessing_summary.json', 'w') as f:\n",
    "    json.dump(preprocessing_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Preprocessing summary saved to: '../data/processed/preprocessing_summary.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80211a22",
   "metadata": {},
   "source": [
    "## Preprocessing Summary & Next Steps\n",
    "The data preprocessing stage successfully addressed missing values, outliers, and inconsistent data types, resulting in a clean and reliable dataset. These efforts ensure that the data is well-prepared for advanced feature engineering and robust model development in the next phases of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
